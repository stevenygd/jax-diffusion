defaults:
  - dit_s

name: "SSM-S"
attn_type: "ssm"
attn_kwargs:
  d_expansion: 1
  d_state: 256

dit_block_kwargs:
  grad_checkpoint_attn: True 
  grad_checkpoint_mlp: True 
  grad_checkpoint_adamlp: False 
  blockwise_mlp: False 
  blockwise_mlp_chunk_size: 128
  # Mixed precision
  mlp_dtype: "bfloat16"
  adaln_mlp_dtype: "bfloat16"

# attn_kwargs:
#   qk_norm: False             # default is False, but divided by self.scale
#   elu: False 
#   normalizer: constant
#   scale_q: False