defaults:
  - dit_s

name: "DiT-S-mt3mlp-v0"
fst_cond_type: "modulate"
snd_cond_type: "modulate"
attn_type: "mttt"
attn_kwargs:
  qk_norm: False            # default is False, but divided by self.scale
  scale_q: False            # Default to not scaling
  mttt_kwargs:
    # Optimization parameters
    n_iters: 1              
    lr: 1.0                 # Inner loop learning rate
    n_epoch: 1              
    learnable_lr: False     # Whether LR is learnable
    learnable_init: True    # Whether initialization is learnable.
    out_ln: False           # Whether use Layernorm in test output of inner opt.
    inp_ln: False           # Whether use Layernorm in input of inner opt.
    trg_ln: False           # Whether use Layernorm in target of the inner opt.
    # Shuffle
    shuffle: True           # Shuffle data (input)
    shuffle_separate: False # Shuffle training label separately.
    shuffle_per_head: True  # Shuffle differently per head/batch.
    # Encoder parameters.
    enc_dim: 256            # Encoder hidden dimension. default=head_dim
    out_dim: null           # Encoder output dimension. default=head_dim
    enc_layers: 2           # Number of encoder layers
    enc_ln: True            # Whether use Layernorm in encoder.
    enc_bias: False         # Whether use bias in encoder.
    # Use residual
    enc_residual: True      # Whether use encoder residual layer.
    enc_residual_bf: False  # Whether use encoder residual apply bf LN.
    # No Bias
    enc_use_bias: False
    enc_bias_init: zeros
    # Xavier initialization
    enc_kernel_init: xavier # Kernel initialization
    # MiSC
    ln_eps: 1e-5            # Epsilon for LayerNorm