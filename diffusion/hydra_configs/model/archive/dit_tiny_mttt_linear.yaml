# Aiming to reproduce linear attention performance.
defaults:
  - dit_tiny

name: "DiT-Ti-mttt-linear"
fst_cond_type: "modulate"
snd_cond_type: "modulate"
attn_type: "mttt"
attn_kwargs:
  qk_norm: False            # default is False, but divided by self.scale
  scale_q: False            # Default to not scaling Q
  mttt_kwargs:
    ln_eps: 1e-5            # Epsilon for LayerNorm
    # Optimization parameters
    n_iters: 1              
    n_epoch: 1              
    lr: 1.0                 # Inner loop learning rate
    learnable_lr: False     # Whether LR is learnable
    learnable_init: False   # Whether initialization is learnable.
    shuffle: False          # Shuffle data (input)
    shuffle_separate: False # Shuffle training label separately.
    out_ln: False           # Whether use Layernorm in test output of inner opt.
    inp_ln: False           # Whether use Layernorm in input of inner opt.
    trg_ln: False           # Whether use Layernorm in target of the inner opt.
    # Encoder parameters.
    enc_dim: null           # Encoder hidden dimension. default=head_dim
    out_dim: null           # Encoder output dimension. default=head_dim
    enc_layers: 1           # Number of encoder layers
    enc_ln: False           # Whether use Layernorm in encoder.
    enc_bias: False         # Whether use bias in encoder.
    enc_residual: False     # Whether use encoder residual layer.
    enc_residual_bf: False  # Whether use encoder residual apply bf LN.
    enc_use_bias: False
    enc_bias_init: zeros
    enc_kernel_init: zeros 