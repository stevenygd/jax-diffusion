defaults:
  - dit_s

name: "SSM-S"
attn_type: "ssm"
attn_kwargs:
  d_expansion: 1
  d_state: 256

# For memory efficiency
grad_checkpoint: False 
scan_blocks: False 
scan_remat: False 
scan_remat_block_size: 6
dit_block_kwargs:
  grad_checkpoint_attn: True 
  grad_checkpoint_mlp: False 
  grad_checkpoint_adamlp: True 
  blockwise_mlp: False 
  blockwise_mlp_chunk_size: 128
  # Mixed precision
  mlp_dtype: "bfloat16"
  adaln_mlp_dtype: "bfloat16"

patch_type: "simple_diffusion"
skip_layers: [0, 1, 2, 3, 4]

# attn_kwargs:
#   qk_norm: False             # default is False, but divided by self.scale
#   elu: False 
#   normalizer: constant
#   scale_q: False